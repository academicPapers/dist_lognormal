---
title: "Distribuição Log-Normal"
subtitle: "Propriedades e aplicações"
author:
- Luiz Fernando Palin Droubi^[SPU/SC, luiz.droubi@planejamento.gov.br]
- Norberto Hochheim^[UFSC, hochheim@gmail.com]
- Willian Zonato^[SPU/SC, willian.zonato@planejamento.gov.br]
date: "`r format(Sys.Date(), '%d/%m/%Y')`"
output:
  pdf_document:
    keep_tex: yes
    number_sections: yes
    includes:
      in_header: preamble.tex
  html_document:
    fig_caption: yes
    keep_md: yes
  word_document: 
    fig_caption: yes
    reference_docx: COBRAC-2018_modelo_artigo_02_08_06_2018.docx
classoption: a4paper
documentclass: article
link-citations: yes
csl: ABNT_UFPR_2011-Mendeley.csl
bibliography: bibliography.bib
header-includes:
  - \usepackage{amsmath,amssymb}
---

```{r, setup, include=FALSE}
library(appraiseR)
library(fitdistrplus)
library(lmtest)
library(sandwich)
library(lindia)
library(mosaic)
library(latticeExtra)
library(stargazer)
library(sjPlot)
library(knitr)
library(kableExtra)
library(ggplot2)
library(latex2exp)
library(reshape2)
library(ggthemes)
theme_set(theme_bw())
trellis.par.set(theme=theEconomist.theme()) # change default color scheme for lattice
knitr::opts_chunk$set(echo = FALSE, fig.path = "images/", dev = "png", dpi = 600, 
                      fig.pos = "H", fig.align = "center", out.width="50%", 
                      tidy=FALSE, size="small")
type <- knitr::opts_knit$get("rmarkdown.pandoc.to")
options(digits = 10)
source("cum_hist.R")
```

# Resumo {-}

Pretende-se com este artigo detalhar o motivo pelo qual a transformação de variável dependente pela função logaritmo é frequentemente adequada na área de avaliação de imóveis. Um procedimento muito comum nesta área é a adoção de transformações para a obtenção de um "melhor" modelo de regressão. A mais usual e preferida de muitos avaliadores é a função logaritmo, especialmente para a variável dependente. Muitas vezes esta transformação é adequada e percebe-se uma notória melhora no ajuste do modelo. Outras vezes, esta transformação pode não ser adequada. Apesar do modelo aparentar-se melhor ajustado, problemas podem ocorrer quanto às verificações das hipóteses clássicas da regressão, as quais nem sempre os avaliadores estão tão atentos quanto estão com as verificações dos intervalos de confiança e níveis de significância. No entanto, o avaliador que assim procede estará verificando intervalos de confiança e níveis de significâncias incorretos, haja vista que a hipótese da heteroscedasticidade implica na incorreção destas inferências. Entendemos que a melhor maneira para apresentar aos avaliadores a importância de criteriosas escolhas de transformações seja através da análise do histograma da variável original e transformada. Normalmente, uma boa escolha de transformação leva à uma distribuição aproximadamente normal. Quando a variável dependente apresenta distribuição lognormal, esta transformação é a transformação logaritmica. Desta maneira, demonstramos as características básicas desta distribuição, sua formulação, características além do seu relacionamento com a distribuição normal. Por fim, demonstramos as implicações da adoção da transformação da variável dependente e abordamos o problema da retransformação da variável dependente à sua escala original.

# INTRODUÇÃO

A transformação de variáveis é um procedimento comum na Engenharia de Avaliações. No entanto, a transformação dos dados por vezes é realizada sem uma análise profunda do comportamento das variáveis. A *Food and Drug Administration* (FDA), órgão federal dos EUA que atua no controle da comercialização de alimentos e medicamentos no país, recomenda:

> A transformação desnecessária de dados deve ser evitada. Caso tenha sido realizada transformação de dados, uma justificativa para a escolha da transformação junto com a interpretação das estimativas dos efeitos do tratamento com base nos dados transformados deve ser fornecida.[@fda apud @keene]

No entanto, a transformação logarítmica é especial, por uma série de aspectos, como pode ser visto em @keene.

A distribuição lognormal apresenta diversas aplicações práticas. É comum, na área de avaliação de imóveis, mas não apenas[^1], nos depararmos com dados que seguem esta distribuição. Neste artigo pretendemos demonstrar as principais características da distribuição lognormal, sua relação com a distribuição normal de Gauss, assim como debatemos a melhor maneira de se lidar com dados lognormais.

[^1]: Dados estritamente positivos, como valores em moeda, altura, peso, etc, normalmente seguem a distribuição lognormal.

# REVISÃO BIBLIOGRÁFICA

## Formulação

A formulação da distribuição lognormal para os parâmetros $\mu$ e $\sigma$ pode ser vista abaixo [@farias]

$$\begin{cases}
f(x;\mu, \sigma) = \frac{1}{x\sigma\sqrt{2\pi}}\exp(-\frac{(log(x) - \mu)^2}{2\sigma^2}) & \forall x > 0 \\ 
0 & \text{ se } x = 0 
\end{cases}$$


## Propriedades

### Valor Esperado e Variância

O valor Esperado $\mathbb{E}$ de uma variável aleatória com distribuição lognormal $X$ é [@farias]:

$$\mathbb{E}(X) = \exp \left (\mu + \frac{\sigma^2}{2} \right )$$
E sua variância é:

$$\newcommand{\Var}{\operatorname{Var}} \Var(X) = \exp (2\mu+\sigma^2)(\exp(\sigma^2)-1)$$


### Medidas de Tendência Central

A figura \ref{fig:densidade_medidas} mostra a posição das medidas de tendência central (moda, média e mediana) para um variável aleatória de distribuição log-normal.

```{r densidade_medidas, fig.cap = "Ilustração das posições de medidas de tendência central numa distribuição lognormal."}
# Medidas de Tendência Central da amostra
mediana <- 1
desvio <- exp(1)
media <- mediana*exp(log(desvio)^2/2)
moda <- mediana/exp(log(desvio)^2)
x <- seq(0, 3, .05)
y <- dlnorm(x, 
            meanlog = log(mediana),
            sdlog = log(desvio))
data <- data.frame(x = x, y = y)
# Gráfico Lognormal
p_logN <- ggplot(data, aes(x = x, y = y)) +
  geom_line() +
  scale_y_continuous(limits = c(0, max(data$y)+.05), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, max(data$x)), expand = c(0, 0)) +
  geom_segment(aes(x = media, y = 0, xend = media, 
                   yend = dlnorm(media, meanlog = log(mediana), sdlog = log(desvio)),
                   colour = "Média")) +
  geom_segment(aes(x = mediana, y = 0, xend = mediana,
                   yend = dlnorm(mediana, meanlog = log(mediana), sdlog = log(desvio)),
                   colour = "Mediana")) +
  geom_segment(aes(x = moda, y = 0, xend = moda, 
                   yend = dlnorm(moda, meanlog = log(mediana), sdlog = log(desvio)),
                   colour = "Moda")) +
  ylab("Densidade") +
  theme(legend.position="bottom", legend.text=element_text(size=6), 
        legend.title=element_text(size=8)) +
  labs(title = "Distribuição Lognormal", subtitle = TeX("$\\mu = log(1)$, $\\sigma = log(e)$"))
p_logN
```

### Efeito das variações do desvio-padrão na forma da distribuição

```{r logs, fig.cap = "Distribuição lognormal com $\\mu = 0$ e diversos valores de $\\sigma$"}
x <- seq(0, 3, 0.01)
sigma <- c(2, 1.5, 1, .5, .25)
y <- lapply(sigma, dlnorm, x = x, meanlog = log(1))
data <- data.frame(x, y[[1]], y[[2]], y[[3]], y[[4]], y[[5]])
colnames(data) <- c("x", "y1", "y2", "y3", "y4", "y5")
data <- melt(data, id = 1)
ggplot(data, aes(x = x, y = value, 
                 color = factor(variable, labels = as.character(sigma)))) +
  geom_line() +
  scale_y_continuous(limits = c(0, max(data$value)), expand = c(0, 0)) + 
  scale_x_continuous(limits = c(0, max(data$x)), expand = c(0, 0)) +
  labs(title = "Distribuições lognormais",
       subtitle = TeX("$\\mu = log(1) = 0$"),
       color = TeX("$\\sigma$"))
```

```{r video, echo = FALSE, fig.show='animate', fig.width=5, fig.height=4, interval=.1}
x <- seq(0, 3, 0.05)
mediana <- 1
sigma <- seq(1.4, 0.1, -0.1)
for (s in sigma) {
  y <- dlnorm(x = x, meanlog = log(1), sdlog = s)
  data <- data.frame(x, y)
  media <- mediana*exp(s^2/2)
  moda <- mediana/exp(s^2)
  p <- ggplot(data, aes(x = x, y = y)) + geom_line() + 
    geom_segment(aes(x = media, y = 0, xend = media, 
                   yend = dlnorm(media, meanlog = log(mediana), sdlog = s),
                   colour = "Média")) +
    geom_segment(aes(x = mediana, y = 0, xend = mediana,
                   yend = dlnorm(mediana, meanlog = log(mediana), sdlog = s),
                   colour = "Mediana")) +
    geom_segment(aes(x = moda, y = 0, xend = moda, 
                   yend = dlnorm(moda, meanlog = log(mediana), sdlog = s),
                   colour = "Moda"))
  print(p)
}
```

### Relação com a distribuição normal

Lembrando que a função densidade de probabilidade de uma variável aleatória com distribuição normal é dada por:

$$f(t) = \frac{1}{\sigma\sqrt{2\pi}}\mathrm{e}^{-\frac{1}{2}\frac{(t-\mu)^2}{\sigma^2}}$$
E que para a distribuição normal-padrão ($N(0,1)$) a função densidade de probabilidade torna-se:

$$\varphi(t) = \frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{1}{2}t^2}$$

Seja $X$ uma variável aleatória de distribuição normal padronizada ($X \sim N(0, 1)$), $f_X$ a função densidade de probabilidade e $Y = e^X$. Então ($F_Y$) é igual a: 

$$F_Y(y) = \mathbb{P}(e^X\leq y) = \mathbb{P}(X \leq ln(Y)) = \int_{-\infty}^{ln(y)}f_X(x)dx = \int_{-\infty}^{ln(y)}\frac{1}{\sqrt{2\pi}}e^{-x^2/2}dx$$
o que equivale a:

$$F_Y(y) = \int_{0}^{y}\frac{1}{x}\frac{1}{\sqrt{2\pi}}e^{-ln(x)^2/2}$$

Ou seja, a distribuição de uma variável $Y = e^X$, em que $X \sim N(0,1)$ é equivalente a distribuição de uma variável lognormal com parâmetros $\mu = 0$ e $\sigma = 1$.

A figura \ref{fig:normal_lognormal} ilustra este fato.

```{r normal_lognormal, out.width="100%", fig.cap = "Comparação entre distribuições normal e lognormal padronizadas."}
# Medidas de Tendência Central da amostra
mediana <- 1
desvio <- exp(1)
media <- mediana*exp(log(desvio)^2/2)
moda <- mediana/exp(log(desvio)^2)
# Gráfico Normal
xvalues <- -4:4
x <- data.frame(x = xvalues)
p_N <- ggplot(x, aes(x = xvalues)) + 
  stat_function(fun = dnorm) + 
  geom_segment(aes(x = 0, y = 0, xend = 0, yend = 0.4, 
                   colour = "Moda = Média = Mediana")) +
  scale_y_continuous(limits = c(0, 0.5), expand = c(0, 0)) +
  scale_x_continuous(limits = c(-4, 4), expand = c(0, 0)) +
  theme(legend.position="bottom", legend.text=element_text(size=6), 
        legend.title=element_text(size=8)) + 
  labs(title = "Normal", subtitle = TeX("$\\mu = log(1) = 0$, $\\sigma = log(e) = 1$"),
       x = "x",
       y = "Densidade")
cowplot::plot_grid(p_logN, p_N, ncol = 2)
```


### Analogia com o Teorema do Limite Central

Assim como o resultado da soma de diversas variáveis independentes com distribuições quaisquer resulta numa variável aleatória de distribuição normal (Teorema do Limite Central), o produto de diversas variáveis aleatórias resulta numa distribuição lognormal.

### Transformação de variável e Homoscedasticidade

De acordo com Matloff [-@matloff2017, 138], se uma variável aleatória $W$ é aproximadamente normal, com baixo coeficiente de variação ($CV = \sigma/\mu$), e $g(W)$ é uma função suave, então a nova variável também será aproximadamente normal, com média $g(EW)$ e variância:

$$[g'(EW)]^2\text{Var}(W)$$

Assumindo que os erros de uma função de regressão sejam heteroscedásticos, seguindo uma função conhecida $\sigma(t) = \mu(t)$, se aplicarmos a função logaritmo natural à variável dependente, segundo a equação acima, teremos[^2]:

$$\frac{1}{\mu^2(t)}\mu^2(t) = 1$$

Ou seja, o uso da transformação logaritmo natural, para este caso em particular, conduz à homoscedasticidade do modelo.

De acordo com Matloff [-@matloff2017, 138], ainda, se $\sigma(t) = \sqrt{\mu(t)}$, a transformação raiz-quadrada é que traria de volta a homoscedasticidade [^3].

[^2]: Lembrando que a derivada da função logaritmo natural é $\frac{d}{dt}\ln t= \frac{1}{t}$ e que $\text{Var}(W) = \sigma^2(W)$
[^3]: $\frac{d}{dt}\sqrt{t} = \frac{0,5}{\sqrt{t}} \rightarrow \text{Var}(\sqrt{W}) = \left (\frac{0,5}{\sqrt{t}} \right)^2(\sqrt{t})^2 = 0,25$

# EXEMPLO

## Dados

Os dados utilizados aqui são oriundos de Hochheim [-@hochheim, 21-22] e são reproduzidos no [ANEXO I].

```{r, echo = FALSE}
dados <- centro_2015@data
```


## Ajuste de distribuições aos dados

```{r fitdist, warning = FALSE, results='hide', fig.show='hold', out.width="49%", fig.cap = "Ajuste da distribuição empírica a diversas distribuições teóricas."}
f1 <- fitdist(dados$valor[1:50], "lnorm", method = "mle")
f2 <- fitdist(dados$valor[1:50], "lnorm", method = "mme")
f3 <- fitdist(dados$valor[1:50], "weibull")
f4 <- fitdist(dados$valor[1:50], "gamma", method = "mme")
plot.legend <- c("lognormal (MLE)", "lognormal (MME)", "weibull", "gamma")
cdfcomp(list(f1, f2, f3, f4), do.points=FALSE, xlogscale = TRUE, main = "CDF plot", legendtext = plot.legend)
denscomp(list(f1, f2, f3, f4), demp=TRUE, main = "Density plot", legendtext = plot.legend)
qqcomp(list(f1, f2, f3, f4), legendtext = plot.legend)
f1
f2
```

Percebe-se pela análise das figuras \ref{fig:fitdist} que o melhor ajuste se deu para a distribuição lognormal ajustada seja pelo método dos momentos (MME) ou pelo método da verossimilhança (MLE), haja vista que as outras distribuições inicialmente crescem mais rapidamente e tem pico mais achatado que os dados empíricos e as distribuições log-normais ajustadas.

## Gráficos

As figuras \ref{fig:densidade} a \ref{fig:hist_densidade2} mostram que os valores observados para a variável \code{valor} do conjunto de dados mencionados acima [@hochheim, 21-22] apresentam distribuição aproximadamente lognormal, com parâmetros $\mu = \bar{ln(valor)}$

\newpage

a. Densidade

A figura \ref{fig:densidade} mostra o gráfico da função densidade de probabilidade (FDP) construídos com os parâmetros $\mu$ e $\sigma$  obtidos da variável valor.

```{r densidade, fig.cap = "Função densidade de probabilidade com parâmetros obtidos dos dados da variável \\code{valor}"}
trellis.par.set(theme=theme.mosaic()) # change default color scheme for lattice
plotDist("lnorm", 
         meanlog = mean(log(dados$valor), na.rm = TRUE),
         sdlog = sd(log(dados$valor), na.rm = TRUE)
         )
```

b. Histograma com densidade superposta

A figura \ref{fig:hist_densidade} mostra o histograma dos dados da variável `valor`, superposto com a curva da função densidade de probabilidade (FDP) da figura \ref{fig:densidade}.

```{r hist_densidade, fig.keep='last', fig.cap = "Histograma das variável \\code{valor} com função densidade de probabilidade superposta."}
histogram(~ valor, data = dados)
plotDist("lnorm", 
         meanlog = mean(log(dados$valor), na.rm = TRUE),
         sdlog = sd(log(dados$valor), na.rm = TRUE), 
         add = TRUE)
```

c. Cumulativa

A figura \ref{fig:cdf} ilustra o gráfico da função cumulativa de probabilidade (FCP) para a variável `valor`.

```{r cdf, fig.keep='last', fig.cap = "Função cumulativa de densidade de probabilidade com parâmetros obtidos dos dados da variável \\code{valor}"}
# dados_ecdf <- ecdf(dados$valor)
# plot(dados_ecdf)
ecdfplot(~valor, data = dados)
plotDist("lnorm", 
         meanlog = mean(log(dados$valor), na.rm = TRUE),
         sdlog = sd(log(dados$valor), na.rm = TRUE), 
         kind = "cdf",
         add = T, col = "red")
```

d. Distribuição da variável $ln(valor)$

A figura \ref{fig:hist_densidade2} mostra a distribuição da variável $\ln(valor)$. Pode-se notar que, conforme esperado, já que a distribuição da variável `valor` é aproximadamente lognormal, seu logaritmo tem distribuição aproximadamente normal.

```{r hist_densidade2, fig.keep='last', fig.cap = "Histograma com função densidade de probabilidade normal superposta"}
histogram(~ log(valor), data = dados, main = "Histograma de ln(valor)", 
          sub = "Com pdf N(mean(ln(valor)), sd(ln(valor)) superposta")
plotDist("norm", 
         mean = mean(log(dados$valor), na.rm = TRUE),
         sd = sd(log(dados$valor), na.rm = TRUE), 
         add = TRUE
         )
```

## Modelos

Detectando-se a presença de variável resposta com distribuição lognormal, pode-se proceder da seguinte maneira:

* Proceder com a transformação da variável resposta pela função logarítmica;
* Proceder com a variável na escala original, corrigindo posteriormente a heteroscedasticidade com o método de Eickert-White;
* Proceder com o Método dos Mínimos Quadrados Ponderados.

### Modelo linear com a variável resposta transformada

É fácil mostrar que o modelo linear com a variável resposta logaritmizada, ou seja, com distribuição normal, é melhor ajustado que o modelo linear de uma variável resposta lognormal. Na tabela \ref{tab:tabela}, no entanto, mostra-se que, para o presente caso, esta melhora de ajuste é modesta, próxima a 4,5%. Além disso, o modelo linear, sem transformação, é heteroscedástico.

```{r}
fit <- lm(valor ~. , data = dados)
bptest(fit)
```

A função máxima verossimilhança de Box-Cox também vai apresentar como transformação ótima a transformação logarítimica, como demonstra a figura \ref{fig:boxcox}

```{r boxcox, fig.cap = "Gráfico da função verossimilhança de Box-Cox"}
gg_boxcox(fit)
```

Na tabela \ref{tab:tabela} é possível comparar os modelos com e sem a transformação da variável resposta. Porém, como o modelo sem transformação é heteroscedástico, os intervalos de confiança dos regressores e os p-valores mostrados na  tabela são inválidos, pois deve-se calcular os erros robustos antes de computá-los, o que será visto na próxima seção.

```{r tabela, results="asis"}
fit1 <- update(fit, log(valor)~.)
fit$AIC <- AIC(fit)
fit1$AIC <- AIC(fit1)
s <- summary(fit1)
stargazer(fit, fit1, header = FALSE, type = type, label = "tab:tabela",
          title = "Comparação entre modelos com e sem transformação da variável resposta",
          ci = TRUE, report = "vcstp*")
```

## Retransformação de variáveis

O problema da transformação da variável resposta no logarítmo da variável resposta original, é que devemos estudar como proceder na retranformação da variável, para efetuar a avaliação do imóvel.

### A desigualdade de Jensen

Segundo Matloff [-@matloff2017, 142], a desigualdade de Jensen (aplicada à estatística) se traduz na seguinte expressão, válida para funções convexas:

$$\mathbb{E}[h(V)] \geq h(\mathbb{E}[V])$$

Isto aplicado no caso da transformação logarítimica, que é uma função côncava, se reduz à expressão abaixo [@matloff2017, 142]:

$$\mathbb{E}[\ln Y|X = t] \leq \ln(E[Y|X = t])$$

Para Matloff, então, como a igualdade só irá acontecer em poucos casos especiais, a função de regressão de $\ln(Y)$ será quase sempre menor do que o logaritmo natural da função de regressão de $Y$, de tal forma que a suposição que dado uma variável aleatória $Y$ tal que assumimos que $E(Y|X = t) = e^{\beta_0 + \beta_1t}$, não podemos concluir de imediato que um modelo linear razoável seria da forma $E(\ln Y|X = t) = \beta_0 + \beta_1t$, pois, pela desigualdade de Jensen, se temos dados significantemente heteroscedásticos da variável original ($Y$), a discrepância entre os dois lados da desigualdade acima poderia variar bastante com $t$, potencialmente produzindo uma grande distorção à forma da curva de regressão [@matloff2017, 143]. Segundo Becker [-@becker, 4], a desigualdade de Jensen pode ser transformada numa igualdade do tipo:

$$\mathbb{E}[f(X)] = f(\mathbb{E}[X]) + \Delta$$
E, de acordo com o mesmo [-@becker, 5], o valor de $\Delta$, chamado de Defeito de Holder, é proporcional à variância da variável aleatória $X$, tal que se $f: [a, b] \rightarrow \mathbb{R}$ é duas vezes continuamente diferenciável e existem limites finitos $m$ e $M$ tais que $0 \leq m \leq f''(x) \leq M$ para todo $x \in [a,b]$, então existe um valor $\mu \in [m, M]$ para o qual a fórmula abaixo é válida:

$$\mathbb{E}[f(X)] - f(\mathbb{E}[X]) = \frac{1}{2}\mu \mathrm{Var} (X)$$
Em suma, o valor de $\Delta$ é proporcional à variância de $X$ ($\Delta \propto \sigma^2(X)$).

Deste modo, existem na literatura diversos estudos sobre qual seria o "melhor" estimador -- paramétrico ou não-paramétrico -- para a variável resposta original, quando da ocorrência da transformação da variável pela função logaritmo natural, como pode ser visto em @Duan, @meulenberg1965 e @shen.

De acordo com Shen e Zhu [-@shen], com a simples aplicação da transformação inversa (exponencial) à aplicada na variável dependente (logarítimica), chegamos ao *Back-Transform* (BT) *Estimator*, que tem performance "muito pior do que os outros estimadores" [-@shen, 554]. De fato, o estimador BT seria mais apropriado para estimar a mediana [@shen, 554], no entanto a equação de regressão linear é uma equação para a média da variável dependente. Métodos de regressão à mediana [@koenker], então, seriam mais apropriados para este fim.

Entendemos que, na precisão necessária para a área de avaliação de imóveis, é suficiente a adoção do estimador teórico, apesar do funcionamento dos estimadores não-paramétricos ter sido demonstrado mais eficiente do que ele.

$$\mathbb{E}(Y|X) = \exp(\beta_0 + \beta_1X + 0.5\sigma^2)$$


### Modelo linear com posterior correção da heteroscedasticidade

#### Erros-padrão robustos

a. Coeficientes

```{r tabela2, results='asis'}
cov <- vcovHC(fit, type = "HC")
robust.se <- sqrt(diag(cov))
stargazer(fit, fit, header = FALSE, type = type, label = "tab:tabela2",
          title = "Comparação entre modelos com e sem erros robustos",
          ci = TRUE, report = "vcstp*", digit.separator = ".", decimal.mark = ",",
          se = list(NULL, robust.se), 
          column.labels = c("default", "robust"))
#coeftest(fit, vcov. = vcovHC)
```

b. Teste F

Na tabela \ref{tab:tabela2}, o resultado para o teste F para o modelo robusto está errado. O teste deve ser refeito com a consideração dos erros robustos:

```{r}
waldtest(fit, vcov = vcovHC)
```

### Mínimos quadrados ponderados

De acordo com Matloff [-@matloff2017, 139], em princípio, o Método dos Mínimos Quadrados ponderados (MQP) fornece melhores estimativas para os coeficientes e inferência estatística correta mesmo na presença de heteroscedasticidade. Tal método consiste, analogamente ao Método dos Mínimos Quadrados Ordinários (MQO), em uma minimização. No caso dos MQO, minimiza-se a quantidade abaixo [@matloff2017, 69]: 

$$\frac{1}{n} \sum_{i=1}^{n} (Y_i - \tilde{X}_i'b)^2$$

Enquanto o MQP minimiza [@matloff2017, 133]:

$$\frac{1}{n} \sum_{i=1}^{n} \frac{1}{w_i} (Y_i - \tilde{X}_i'b)^2$$

Onde $w_i = \sigma^2(X_i)$.


### Comparação das previsões para os dois modelos 

```{r}
p <- predict(fit1, interval = "confidence", level = 0.80)
p <- as.data.frame(exp(p + .5*s$sigma^2))
p$amplitude <- (p$upr-p$lwr)/p$fit
```

```{r}
source("robust_predictions.R")
p_rob <- robust_predictions(fit, rob = TRUE, level = 0.80)
p_rob$amplitude <- (p_rob$ci.upper - p_rob$ci.lower)/p_rob$predicted.value
```

```{r}
kable(list(p, p_rob), 
      format = ifelse(type == "html", "markdown", "latex"),
      booktabs = TRUE,
      digits = 2, format.args = list(big.mark = ".", decimal.mark = ",")) 
```


# CONCLUSÃO

Foi possível demonstrar de maneira gráfica que os dados da variável valor apresentados se ajustam bem a uma distribuição lognormal equivalente. Por definição, então, o logaritmo da variável possui distribuição normal.

Consideramos que o valor mais provável para a variável resposta é o seu Valor Esperado. Logo, a retransformação da variável deve ser feita para a média da variável log-normal.

Uma alternativa seria a aplicação da regressão linear ponderada (ou mínimos quadrados ponderados). No entanto, o método de 
Eickert-White parece ser adequado e sua aplicação menos complexa do que o método de regressão ponderada.
\newpage

# ANEXO I {-}

```{r, echo = FALSE}
kable(dados[complete.cases(dados),], 
      format = ifelse(type == "html", "markdown", "latex"),
      booktabs = TRUE) %>%
  kable_styling(latex_options = "striped") 
```

# REFERÊNCIAS {-}
